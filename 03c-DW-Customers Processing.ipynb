{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangler Processing Job for Customers Dataset\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Quick Start </strong>\n",
    "To save your processed data to S3, select the Run menu above and click <strong>Run all cells</strong>. \n",
    "<strong><a style=\"color: #0397a7 \" href=\"#Job-Status-&-S3-Output-Location\">\n",
    "    <u>View the status of the export job and the output S3 location</u></a>.\n",
    "</strong>\n",
    "</div>\n",
    "\n",
    "\n",
    "This notebook executes your Data Wrangler Flow `customers.flow` on the entire dataset using a SageMaker \n",
    "Processing Job and will save the processed data to S3.\n",
    "\n",
    "This notebook saves data from the step `Cast Single Data Type` from `Source: customers.Csv`. To save from a different step, go to Data Wrangler \n",
    "to select a new step to export. \n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Inputs and Outputs](#Inputs-and-Outputs)\n",
    "1. [Run Processing Job](#Run-Processing-Job)\n",
    "   1. [Job Configurations](#Job-Configurations)\n",
    "   1. [Create Processing Job](#Create-Processing-Job)\n",
    "   1. [Job Status & S3 Output Location](#Job-Status-&-S3-Output-Location)\n",
    "1. [Optional Next Steps](#(Optional)Next-Steps)\n",
    "    1. [Load Processed Data into Pandas](#(Optional)-Load-Processed-Data-into-Pandas)\n",
    "    1. [Train a model with SageMaker](#(Optional)Train-a-model-with-SageMaker)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading stored variables\n",
    "If you ran this notebook before, you may want to re-use the resources you aready created with AWS. Run the cell below to load any prevously created variables. You should see a print-out of the existing variables. If you don't see anything printed then it's probably the first time you are running the notebook! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "region = sagemaker.Session().boto_region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "sess = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "bucket                             -> 'sagemaker-us-east-1-875692608981'\n",
      "claims_fg_name                     -> 'fraud-detect-demo-claims'\n",
      "claims_preprocessed                ->       policy_id  incident_severity  num_vehicles_i\n",
      "claims_table                       -> 'fraud-detect-demo-claims-1636518800'\n",
      "clarify_expl_job_name              -> 'Clarify-Explainability-2021-11-10-14-35-21-747'\n",
      "col_order                          -> ['fraud', 'num_injuries', 'incident_severity', 'in\n",
      "customers_fg_name                  -> 'fraud-detect-demo-customers'\n",
      "customers_preprocessed             ->       policy_id  customer_age  customer_education \n",
      "customers_table                    -> 'fraud-detect-demo-customers-1636518803'\n",
      "database_name                      -> 'sagemaker_featurestore'\n",
      "endpoint_config_name               -> 'fraud-detect-demo-xgboost-post-smote-endpoint-con\n",
      "endpoint_name                      -> 'fraud-detect-demo-2021-11-11-08-19-15'\n",
      "hyperparameters                    -> {'max_depth': '3', 'eta': '0.2', 'objective': 'bin\n",
      "model_1_name                       -> 'fraud-detect-demo-xgboost-pre-smote'\n",
      "model_2_name                       -> 'fraud-detect-demo-xgboost-post-smote'\n",
      "mp2_arn                            -> 'arn:aws:sagemaker:us-east-1:875692608981:model-pa\n",
      "mpg_name                           -> 'fraud-detect-demo'\n",
      "prefix                             -> 'fraud-detect-demo'\n",
      "test_data_uri                      -> 's3://sagemaker-us-east-1-875692608981/fraud-detec\n",
      "train                              ->       fraud  num_injuries  incident_severity  inju\n",
      "train_data_uri                     -> 's3://sagemaker-us-east-1-875692608981/fraud-detec\n",
      "training_job_1_name                -> 'sagemaker-xgboost-2021-11-10-04-56-34-757'\n",
      "training_job_2_name                -> 'sagemaker-xgboost-2021-11-10-14-28-13-883'\n",
      "training_job_3_name                -> 'sagemaker-xgboost-2021-11-11-02-27-24-634'\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======> Tons of output_paths\n",
    "processing_dir = \"/opt/ml/processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3\n",
    "Before you can preprocess the raw data with Data Wrangler, it must exist in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"data/customers.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/customers.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Outputs\n",
    "\n",
    "The below settings configure the inputs and outputs for the flow export.\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Configurable Settings </strong>\n",
    "\n",
    "In <b>Input - Source</b> you can configure the data sources that will be used as input by Data Wrangler\n",
    "\n",
    "1. For S3 sources, configure the source attribute that points to the input S3 prefixes\n",
    "2. For all other sources, configure attributes like query_string, database in the source's \n",
    "<b>DatasetDefinition</b> object.\n",
    "\n",
    "If you modify the inputs the provided data must have the same schema and format as the data used in the Flow. \n",
    "You should also re-execute the cells in this section if you have modified the settings in any data sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "\n",
    "data_sources = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input - S3 Source: customers.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sources.append(ProcessingInput(\n",
    "    source=f\"s3://{bucket}/{prefix}/data/raw/customers.csv\", # You can override this to point to other dataset on S3\n",
    "    destination=f\"{processing_dir}/customers.csv\",\n",
    "    input_name=\"customers.csv\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: S3 settings\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Configurable Settings </strong>\n",
    "\n",
    "1. <b>bucket</b>: you can configure the S3 bucket where Data Wrangler will save the output. The default bucket from \n",
    "the SageMaker notebook session is used. \n",
    "2. <b>flow_export_id</b>: A randomly generated export id. The export id must be unique to ensure the results do not \n",
    "conflict with other flow exports \n",
    "3. <b>s3_ouput_prefix</b>:  you can configure the directory name in your bucket where your data will be saved.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow export name: flow-11-12-14-06-75409070\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "\n",
    "# unique flow export ID\n",
    "flow_export_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\"\n",
    "print(f\"Flow export name: {flow_export_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the inputs required by the SageMaker Python SDK to launch a processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flow file from current notebook working directory: /root/mlt\n",
      "Output name: dc0ba3db-3a12-49ef-8b39-a7b7867e295b.default\n",
      "Flow S3 export result path: s3://sagemaker-us-east-1-875692608981/fraud-detect-demo/flow/export-flow-11-12-14-06-75409070/output\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"flows/customers.flow\"\n",
    "\n",
    "# Load .flow file from current notebook working directory \n",
    "!echo \"Loading flow file from current notebook working directory: $PWD\"\n",
    "\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "output_name = (f\"{flow['nodes'][-1]['node_id']}.{flow['nodes'][-1]['outputs'][0]['name']}\")\n",
    "print(f\"Output name: {output_name}\")\n",
    "\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{prefix}/flow/{s3_output_prefix}\"\n",
    "print(f\"Flow S3 export result path: {s3_output_path}\")\n",
    "\n",
    "processing_job_output = ProcessingOutput(\n",
    "    output_name=output_name,\n",
    "    source=\"/opt/ml/processing/output\",\n",
    "    destination=s3_output_path,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Flow to S3\n",
    "\n",
    "To use the Data Wrangler as an input to the processing job,  first upload your flow file to Amazon S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangler flow flows/customers.flow uploaded to s3://sagemaker-us-east-1-875692608981/fraud-detect-demo/data_wrangler_flows/flow-11-12-14-06-75409070.flow\n"
     ]
    }
   ],
   "source": [
    "# Upload flow to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"{prefix}/data_wrangler_flows/{flow_export_name}.flow\", ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n",
    "\n",
    "flow_s3_uri = f\"s3://{bucket}/{prefix}/data_wrangler_flows/{flow_export_name}.flow\"\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Wrangler Flow is also provided to the Processing Job as an input source which we configure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Input - Flow: claims.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=f\"{processing_dir}/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Processing Job \n",
    "## Job Configurations\n",
    "\n",
    "<div class=\"alert alert-info\"> ðŸ’¡ <strong> Configurable Settings </strong>\n",
    "\n",
    "You can configure the following settings for Processing Jobs. If you change any configurations you will \n",
    "need to re-execute this and all cells below it by selecting the Run menu above and click \n",
    "<b>Run Selected Cells and All Below</b>\n",
    "\n",
    "1. IAM role for executing the processing job. \n",
    "2. A unique name of the processing job. Give a unique name every time you re-execute processing jobs\n",
    "3. Data Wrangler Container URL.\n",
    "4. Instance count, instance type and storage volume size in GB.\n",
    "5. Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "6. Network Isolation settings\n",
    "7. KMS key to encrypt output data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container uri: 663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris\n",
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Unique processing job name. Give a unique name every time you re-execute processing jobs\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n",
    "\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = image_uris.retrieve(framework='data-wrangler',region=region)\n",
    "print(f\"Container uri: {container_uri}\")\n",
    "# Pinned Data Wrangler Container URL. \n",
    "# container_uri_pinned = \"663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.11.2\"\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 2\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing\n",
    "volume_size_in_gb = 30\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Network Isolation mode; default is off\n",
    "enable_network_isolation = False\n",
    "\n",
    "# Output configuration used as processing job container arguments \n",
    "output_config = {\n",
    "    output_name: {\n",
    "        \"content_type\": output_content_type\n",
    "    }\n",
    "}\n",
    "\n",
    "# KMS key for per object encryption; default is None\n",
    "kms_key = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processing Job\n",
    "\n",
    "To launch a Processing Job, you will use the SageMaker Python SDK to create a Processor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  data-wrangler-flow-processing-11-12-14-06-75409070\n",
      "Inputs:  [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-875692608981/fraud-detect-demo/data_wrangler_flows/flow-11-12-14-06-75409070.flow', 'LocalPath': '/opt/ml/processing/flow', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'dc0ba3db-3a12-49ef-8b39-a7b7867e295b.default', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-875692608981/fraud-detect-demo/flow/export-flow-11-12-14-06-75409070/output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=NetworkConfig(enable_network_isolation=enable_network_isolation),\n",
    "    sagemaker_session=sess,\n",
    "    output_kms_key=kms_key\n",
    ")\n",
    "\n",
    "# Start Job\n",
    "processor.run(\n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[processing_job_output],\n",
    "    arguments=[f\"--output-config '{json.dumps(output_config)}'\"],\n",
    "    wait=False,\n",
    "    logs=False,\n",
    "    job_name=processing_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Status & S3 Output Location\n",
    "\n",
    "Below you wait for processing job to finish. If it finishes successfully, the raw parameters used by the \n",
    "Processing Job will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job results are saved to S3 path: s3://sagemaker-us-east-1-875692608981/export-flow-11-12-14-06-75409070/output/data-wrangler-flow-processing-11-12-14-06-75409070\n",
      ".......................................................................!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'flow',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-875692608981/fraud-detect-demo/data_wrangler_flows/flow-11-12-14-06-75409070.flow',\n",
       "    'LocalPath': '/opt/ml/processing/flow',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'dc0ba3db-3a12-49ef-8b39-a7b7867e295b.default',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-875692608981/fraud-detect-demo/flow/export-flow-11-12-14-06-75409070/output',\n",
       "     'LocalPath': '/opt/ml/processing/output',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'data-wrangler-flow-processing-11-12-14-06-75409070',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2,\n",
       "   'InstanceType': 'ml.m5.4xlarge',\n",
       "   'VolumeSizeInGB': 30}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       " 'AppSpecification': {'ImageUri': '663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x',\n",
       "  'ContainerArguments': ['--output-config \\'{\"dc0ba3db-3a12-49ef-8b39-a7b7867e295b.default\": {\"content_type\": \"CSV\"}}\\'']},\n",
       " 'NetworkConfig': {'EnableInterContainerTrafficEncryption': False,\n",
       "  'EnableNetworkIsolation': False},\n",
       " 'RoleArn': 'arn:aws:iam::875692608981:role/AmazonSageMaker-ExecutionRoleMLT',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:875692608981:processing-job/data-wrangler-flow-processing-11-12-14-06-75409070',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ProcessingEndTime': datetime.datetime(2021, 11, 11, 12, 23, 31, 731000, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2021, 11, 11, 12, 22, 0, 96000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 11, 11, 12, 23, 32, 66000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2021, 11, 11, 12, 17, 35, 214000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '65fa494e-ee56-49e4-b588-3bb6fb118972',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '65fa494e-ee56-49e4-b588-3bb6fb118972',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '1783',\n",
       "   'date': 'Thu, 11 Nov 2021 12:23:35 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_job_results_path = f\"s3://{bucket}/{prefix}/flow/{s3_output_prefix}/{processing_job_name}\"\n",
    "print(f\"Job results are saved to S3 path: {s3_job_results_path}\")\n",
    "\n",
    "job_result = sess.wait_for_processing_job(processing_job_name)\n",
    "job_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-datasets'></a>\n",
    "\n",
    "## DataSets and Feature Types\n",
    "[overview](#all-up-overview)\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_dtypes = {\n",
    "    \"policy_id\": int,\n",
    "    \"customer_age\": int,\n",
    "    \"customer_education\": int,\n",
    "    \"months_as_customer\": int,\n",
    "    \"policy_deductable\": int,\n",
    "    \"policy_annual_premium\": int,\n",
    "    \"policy_liability\": int,\n",
    "    \"auto_year\": int,\n",
    "    \"num_claims_past_year\": int,\n",
    "    \"num_insurers_past_5_years\": int,\n",
    "    \"customer_gender_male\": int,\n",
    "    \"customer_gender_female\": int,\n",
    "    \"policy_state_ca\": int,\n",
    "    \"policy_state_wa\": int,\n",
    "    \"policy_state_az\": int,\n",
    "    \"policy_state_or\": int,\n",
    "    \"policy_state_nv\": int,\n",
    "    \"policy_state_id\": int,\n",
    "    \"event_time\": float,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Processed Data into Pandas\n",
    "\n",
    "We use the [AWS Data Wrangler library](https://github.com/awslabs/aws-data-wrangler) to load the exported \n",
    "dataset into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'customers_preprocessed' (DataFrame)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>customer_age</th>\n",
       "      <th>customer_education</th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>policy_liability</th>\n",
       "      <th>auto_year</th>\n",
       "      <th>num_claims_past_year</th>\n",
       "      <th>num_insurers_past_5_years</th>\n",
       "      <th>customer_gender_male</th>\n",
       "      <th>customer_gender_female</th>\n",
       "      <th>policy_state_ca</th>\n",
       "      <th>policy_state_wa</th>\n",
       "      <th>policy_state_az</th>\n",
       "      <th>policy_state_or</th>\n",
       "      <th>policy_state_nv</th>\n",
       "      <th>policy_state_id</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>94</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>2006</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.636633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>41</td>\n",
       "      <td>3</td>\n",
       "      <td>165</td>\n",
       "      <td>750</td>\n",
       "      <td>2950</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.636633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>57</td>\n",
       "      <td>3</td>\n",
       "      <td>155</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>2017</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.636633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>39</td>\n",
       "      <td>4</td>\n",
       "      <td>80</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.636633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>0</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.636633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4996</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>750</td>\n",
       "      <td>2550</td>\n",
       "      <td>1</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.636633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4997</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>150</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>2</td>\n",
       "      <td>2015</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.636633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4998</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>87</td>\n",
       "      <td>750</td>\n",
       "      <td>2950</td>\n",
       "      <td>1</td>\n",
       "      <td>2016</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.636633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4999</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>750</td>\n",
       "      <td>3000</td>\n",
       "      <td>1</td>\n",
       "      <td>2018</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.636633e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5000</td>\n",
       "      <td>66</td>\n",
       "      <td>4</td>\n",
       "      <td>298</td>\n",
       "      <td>750</td>\n",
       "      <td>2750</td>\n",
       "      <td>1</td>\n",
       "      <td>2020</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.636633e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      policy_id  customer_age  customer_education  months_as_customer  \\\n",
       "0             1            54                   2                  94   \n",
       "1             2            41                   3                 165   \n",
       "2             3            57                   3                 155   \n",
       "3             4            39                   4                  80   \n",
       "4             5            39                   1                  60   \n",
       "...         ...           ...                 ...                 ...   \n",
       "4995       4996            33                   2                   4   \n",
       "4996       4997            45                   3                 150   \n",
       "4997       4998            28                   1                  87   \n",
       "4998       4999            21                   1                   5   \n",
       "4999       5000            66                   4                 298   \n",
       "\n",
       "      policy_deductable  policy_annual_premium  policy_liability  auto_year  \\\n",
       "0                   750                   3000                 1       2006   \n",
       "1                   750                   2950                 0       2012   \n",
       "2                   750                   3000                 0       2017   \n",
       "3                   750                   3000                 2       2020   \n",
       "4                   750                   3000                 0       2018   \n",
       "...                 ...                    ...               ...        ...   \n",
       "4995                750                   2550                 1       2015   \n",
       "4996                750                   3000                 2       2015   \n",
       "4997                750                   2950                 1       2016   \n",
       "4998                750                   3000                 1       2018   \n",
       "4999                750                   2750                 1       2020   \n",
       "\n",
       "      num_claims_past_year  num_insurers_past_5_years  customer_gender_male  \\\n",
       "0                        0                          1                     0   \n",
       "1                        0                          1                     1   \n",
       "2                        0                          1                     0   \n",
       "3                        0                          1                     0   \n",
       "4                        0                          1                     0   \n",
       "...                    ...                        ...                   ...   \n",
       "4995                     0                          3                     1   \n",
       "4996                     0                          1                     0   \n",
       "4997                     0                          1                     1   \n",
       "4998                     0                          2                     0   \n",
       "4999                     0                          1                     1   \n",
       "\n",
       "      customer_gender_female  policy_state_ca  policy_state_wa  \\\n",
       "0                          0                0                1   \n",
       "1                          0                1                0   \n",
       "2                          1                1                0   \n",
       "3                          1                0                0   \n",
       "4                          1                1                0   \n",
       "...                      ...              ...              ...   \n",
       "4995                       0                1                0   \n",
       "4996                       1                0                0   \n",
       "4997                       0                1                0   \n",
       "4998                       1                1                0   \n",
       "4999                       0                1                0   \n",
       "\n",
       "      policy_state_az  policy_state_or  policy_state_nv  policy_state_id  \\\n",
       "0                   0                0                0                0   \n",
       "1                   0                0                0                0   \n",
       "2                   0                0                0                0   \n",
       "3                   1                0                0                0   \n",
       "4                   0                0                0                0   \n",
       "...               ...              ...              ...              ...   \n",
       "4995                0                0                0                0   \n",
       "4996                0                0                1                0   \n",
       "4997                0                0                0                0   \n",
       "4998                0                0                0                0   \n",
       "4999                0                0                0                0   \n",
       "\n",
       "        event_time  \n",
       "0     1.636633e+09  \n",
       "1     1.636633e+09  \n",
       "2     1.636633e+09  \n",
       "3     1.636633e+09  \n",
       "4     1.636633e+09  \n",
       "...            ...  \n",
       "4995  1.636633e+09  \n",
       "4996  1.636633e+09  \n",
       "4997  1.636633e+09  \n",
       "4998  1.636633e+09  \n",
       "4999  1.636633e+09  \n",
       "\n",
       "[5000 rows x 19 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======> This is your DataFlow output path if you decide to redo the work in DataFlow on your own\n",
    "if output_content_type.upper() == \"CSV\":\n",
    "    customers_preprocessed = wr.s3.read_csv(\n",
    "        path=s3_output_path, dataset=True, dtype=customers_dtypes\n",
    "    )\n",
    "else:\n",
    "    print(f\"Unexpected output content type {output_content_type}\")\n",
    "\n",
    "%store customers_preprocessed\n",
    "customers_preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a set of Pandas DataFrames that contain the customer and claim data, with the correct data types. When Dat Wrangler encodes a feature as one-hot-encoded feature, it will default to float data types for those resulting features (one feature --> many columns for the one hot encoding). \n",
    "\n",
    "<font color ='red'> Note: </font> the reason for explicitly converting the data types for categorical features generated by Data Wrangler, is to ensure they are of type integer so that Clarify will treat them as categorical variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional)Train a model with SageMaker\n",
    "Now that the data has been processed, you may want to train a model using the data. The following shows an \n",
    "example of doing so using a popular algorithm - XGBoost. For more information on algorithms available in \n",
    "SageMaker, see [Getting Started with SageMaker Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html). \n",
    "It is important to note that the following XGBoost objective ['binary', 'regression', 'multiclass'] \n",
    "hyperparameters, or content_type may not be suitable for the output data, and will require changes to \n",
    "train a proper model. Furthermore, for CSV training, the algorithm assumes that the target \n",
    "variable is in the first column. For more information on SageMaker XGBoost, \n",
    "see https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html.\n",
    "\n",
    "\n",
    "### Set Training Data path\n",
    "We set the training input data path from the output of the Data Wrangler processing job.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_training_input_path = s3_job_results_path\n",
    "print(f\"training input data path: {s3_training_input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the algorithm and training job\n",
    "\n",
    "The Training Job hyperparameters are set. For more information on XGBoost Hyperparameters, \n",
    "see https://xgboost.readthedocs.io/en/latest/parameter.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "container = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n",
    "hyperparameters = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"num_round\": \"10\",\n",
    "}\n",
    "train_content_type = (\n",
    "    \"application/x-parquet\" if output_content_type.upper() == \"PARQUET\"\n",
    "    else \"text/csv\"\n",
    ")\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    s3_data=s3_training_input_path,\n",
    "    content_type=train_content_type,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start the Training Job\n",
    "\n",
    "The TrainingJob configurations are set using the SageMaker Python SDK Estimator, and which is fit using \n",
    "the training data from the Processing Job that was run earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    iam_role,\n",
    "    hyperparameters=hyperparameters,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.2xlarge\",\n",
    ")\n",
    "estimator.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a trained model there are a number of different things you can do. \n",
    "For more details on training with SageMaker, please see \n",
    "https://sagemaker.readthedocs.io/en/stable/frameworks/xgboost/using_xgboost.html."
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
