{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Data Preparation, Process, and Store Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='all-up-overview'></a>\n",
    "\n",
    "## [Overview](./0-AutoClaimFraudDetection.ipynb)\n",
    "* Notebook 1/2: Overview, Architecture and Data Exploration\n",
    "* **[Notebook 3: Data Preparation, Process, and Store Features](./1-data-prep-e2e.ipynb)**\n",
    "  * **[Architecture](#arch)**\n",
    "  * **[Getting started](#aud-getting-started)**\n",
    "  * **[DataSets](#aud-datasets)**\n",
    "  * **[SageMaker Feature Store](#aud-feature-store)**\n",
    "  * **[Create train and test datasets](#aud-dataset)**\n",
    "* Notebook 4: Train, Check Bias, Tune, Record Lineage, and Register a Model\n",
    "* Notebook 5: Mitigate Bias, Train New Model, Store in Registry)\n",
    "* Notebook 6: Deploy Model, Run Predictions\n",
    "* Notebook 7: Create and Run an End-to-End Pipeline to Deploy the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to perform the Data Prep phase of the ML life cycle. The main Data Wrangling, data ingestion, and multiple transformations will be done through the SageMaker Studio Data Wrangler GUI.\n",
    "\n",
    "In this notebook, we will take the `.flow` files that define the transformations to the raw data. and apply them using a SageMaker Processing job that will apply those transformations to the raw data deposited in the S3 bucket as `.csv` files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='arch'> </a>\n",
    "\n",
    "## Architecture for Data Prep, Process and Store Features\n",
    "[overview](#all-up-overview)\n",
    "----\n",
    "![Data Prep and Store](./images/e2e-1-pipeline-v3b.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install required and/or update third-party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Requirement already satisfied: sagemaker in /opt/conda/lib/python3.7/site-packages (2.66.0)\n",
      "Collecting sagemaker\n",
      "  Using cached sagemaker-2.68.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: google-pasta in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.5.0)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.20.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (20.1)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.3.4)\n",
      "Requirement already satisfied: pathos in /opt/conda/lib/python3.7/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: protobuf>=3.1 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (3.19.0)\n",
      "Requirement already satisfied: attrs in /opt/conda/lib/python3.7/site-packages (from sagemaker) (19.3.0)\n",
      "Requirement already satisfied: boto3>=1.16.32 in /opt/conda/lib/python3.7/site-packages (from sagemaker) (1.19.3)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.23.0,>=1.22.3 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (1.22.3)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3>=1.16.32->sagemaker) (0.5.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (2.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (2.4.6)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->sagemaker) (1.14.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2021.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->sagemaker) (2.8.1)\n",
      "Requirement already satisfied: dill>=0.3.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: pox>=0.3.0 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /opt/conda/lib/python3.7/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<1.23.0,>=1.22.3->boto3>=1.16.32->sagemaker) (1.26.7)\n",
      "Installing collected packages: sagemaker\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.66.0\n",
      "    Uninstalling sagemaker-2.66.0:\n",
      "      Successfully uninstalled sagemaker-2.66.0\n",
      "Successfully installed sagemaker-2.68.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!python -m pip install -Uq pip\n",
    "!python -m pip install -q awswrangler imbalanced-learn\n",
    "!python -m pip install --upgrade sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading stored variables\n",
    "If you ran this notebook before, you may want to re-use the resources you aready created with AWS. Run the cell below to load any prevously created variables. You should see a print-out of the existing variables. If you don't see anything printed then it's probably the first time you are running the notebook! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "bucket                             -> 'sagemaker-us-east-1-875692608981'\n",
      "claims_fg_name                     -> 'fraud-detect-demo-claims'\n",
      "claims_preprocessed                ->       policy_id  incident_severity  num_vehicles_i\n",
      "claims_table                       -> 'fraud-detect-demo-claims-1636518800'\n",
      "clarify_expl_job_name              -> 'Clarify-Explainability-2021-11-10-14-35-21-747'\n",
      "col_order                          -> ['fraud', 'num_injuries', 'incident_severity', 'in\n",
      "customers_fg_name                  -> 'fraud-detect-demo-customers'\n",
      "customers_preprocessed             ->       policy_id  customer_age  customer_education \n",
      "customers_table                    -> 'fraud-detect-demo-customers-1636518803'\n",
      "database_name                      -> 'sagemaker_featurestore'\n",
      "endpoint_config_name               -> 'fraud-detect-demo-xgboost-post-smote-endpoint-con\n",
      "endpoint_name                      -> 'fraud-detect-demo-2021-11-11-08-19-15'\n",
      "hyperparameters                    -> {'max_depth': '3', 'eta': '0.2', 'objective': 'bin\n",
      "model_1_name                       -> 'fraud-detect-demo-xgboost-pre-smote'\n",
      "model_2_name                       -> 'fraud-detect-demo-xgboost-post-smote'\n",
      "mp2_arn                            -> 'arn:aws:sagemaker:us-east-1:875692608981:model-pa\n",
      "mpg_name                           -> 'fraud-detect-demo'\n",
      "prefix                             -> 'fraud-detect-demo'\n",
      "test_data_uri                      -> 's3://sagemaker-us-east-1-875692608981/fraud-detec\n",
      "train                              ->       fraud  num_injuries  incident_severity  inju\n",
      "train_data_uri                     -> 's3://sagemaker-us-east-1-875692608981/fraud-detec\n",
      "training_job_1_name                -> 'sagemaker-xgboost-2021-11-10-04-56-34-757'\n",
      "training_job_2_name                -> 'sagemaker-xgboost-2021-11-10-14-28-13-883'\n",
      "training_job_3_name                -> 'sagemaker-xgboost-2021-11-11-02-27-24-634'\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<font color='red'>Important</font>: You must have run the previous sequential notebooks to retrieve variables using the StoreMagic command.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import string\n",
    "import sagemaker\n",
    "import pandas as pd\n",
    "import awswrangler as wr\n",
    "\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-getting-started'></a>\n",
    "\n",
    "## Getting started: Creating Resources\n",
    "\n",
    "[overview](#all-up-overview)\n",
    "----\n",
    "In order to successfully run this notebook you will need to create some AWS resources. \n",
    "First, an S3 bucket will be created to store all the data for this tutorial. \n",
    "Once created, you will then need to create an AWS Glue role using the IAM console then attach a policy to the S3 bucket to allow FeatureStore access to this notebook. If you've already run this notebook and are picking up where you left off, then running the cells below should pick up the resources you already created without creating any additional resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add FeatureStore policy to Studio's execution role\n",
    "\n",
    "![title](images/iam-policies.png)\n",
    "\n",
    "\n",
    "1. In a separate brower tab go to the IAM section of the AWS Console\n",
    "2. Navigate to the Roles section and select the execution role you're using for your SageMaker Studio user\n",
    "    * If you're not sure what role you're using, run the cell below to print it out\n",
    "3. Attach the <font color='green'> AmazonSageMakerFeatureStoreAccess </font> policy to this role. Once attached, the changes take  effect immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker Role: AmazonSageMaker-ExecutionRoleMLT\n"
     ]
    }
   ],
   "source": [
    "print(\"SageMaker Role:\", sagemaker.get_execution_role().split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set region, boto3 and SageMaker SDK variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using AWS Region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# You can change this to a region of your choice\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "print(\"Using AWS Region: {}\".format(region))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.setup_default_session(region_name=region)\n",
    "\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " sagemaker_role : arn:aws:iam::875692608981:role/AmazonSageMaker-ExecutionRoleMLT\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Note: if you are not running this notebook from SageMaker Studio or SageMaker Classic Notebooks you will need to instanatiate \n",
    "the sagemaker_execution_role_name with an AWS role that has SageMakerFullAccess and SageMakerFeatureStoreFullAccess\n",
    "\"\"\"\n",
    "sagemaker_execution_role_name = \"AmazonSageMaker-ExecutionRole-20210107T234882\"\n",
    "try:\n",
    "    sagemaker_role = sagemaker.get_execution_role()\n",
    "    print(f\"\\n sagemaker_role : {sagemaker_role}\")\n",
    "except ValueError:\n",
    "    iam = boto3.client(\"iam\")\n",
    "    sagemaker_role = iam.get_role(RoleName=sagemaker_execution_role_name)[\"Role\"][\"Arn\"]\n",
    "    print(f\"\\n instantiating sagemaker_role with supplied role name : {sagemaker_role}\")\n",
    "\n",
    "account_id = boto3.client(\"sts\").get_caller_identity()[\"Account\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a directory in the SageMaker default bucket for this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'bucket' not in locals():\n",
    "    bucket = sagemaker_session.default_bucket()\n",
    "    prefix = 'fraud-detect-demo'\n",
    "    %store bucket\n",
    "    %store prefix\n",
    "    print(f'Creating bucket: {bucket}...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use your own S3 bucket that's already existing, uncomment and utilize the following example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntry:\\n    s3_client.create_bucket(Bucket=bucket, ACL='private', CreateBucketConfiguration={'LocationConstraint': region})\\n    print('Create S3 bucket: SUCCESS')\\n    \\nexcept Exception as e:\\n    if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\\n        print(f'Using existing bucket: {bucket}/{prefix}')\\n    else:\\n        raise(e)\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "try:\n",
    "    s3_client.create_bucket(Bucket=bucket, ACL='private', CreateBucketConfiguration={'LocationConstraint': region})\n",
    "    print('Create S3 bucket: SUCCESS')\n",
    "    \n",
    "except Exception as e:\n",
    "    if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':\n",
    "        print(f'Using existing bucket: {bucket}/{prefix}')\n",
    "    else:\n",
    "        raise(e)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======> Tons of output_paths\n",
    "traing_job_output_path = f\"s3://{bucket}/{prefix}/training_jobs\"\n",
    "bias_report_1_output_path = f\"s3://{bucket}/{prefix}/clarify-bias-1\"\n",
    "bias_report_2_output_path = f\"s3://{bucket}/{prefix}/clarify-bias-2\"\n",
    "explainability_output_path = f\"s3://{bucket}/{prefix}/clarify-explainability\"\n",
    "processing_dir = \"/opt/ml/processing\"\n",
    "claims_flow_uri = f\"s3://{bucket}/{prefix}/flow/claims.flow\"\n",
    "customers_flow_uri = f\"s3://{bucket}/{prefix}/flow/customers.flow\"\n",
    "train_data_uri = f\"s3://{bucket}/{prefix}/data/train/train.csv\"\n",
    "test_data_uri = f\"s3://{bucket}/{prefix}/data/test/test.csv\"\n",
    "\n",
    "# =======> variables used for parameterizing the notebook run\n",
    "train_instance_count = 1\n",
    "train_instance_type = \"ml.m4.xlarge\"\n",
    "\n",
    "claify_instance_count = 1\n",
    "clairfy_instance_type = \"ml.c5.xlarge\"\n",
    "\n",
    "predictor_instance_count = 1\n",
    "predictor_instance_type = \"ml.c5.xlarge\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3\n",
    "Before you can preprocess the raw data with Data Wrangler, it must exist in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"data/claims.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/claims.csv\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    Filename=\"data/customers.csv\", Bucket=bucket, Key=f\"{prefix}/data/raw/customers.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update attributes within the  `.flow` file \n",
    "DataWrangler will generate a .flow file. It contains a reference to an S3 bucket used during the Wrangling. This may be different from the one you have as a default in this notebook eg if the Wrangling was done by someone else, you will probably not have access to their bucket and you now need to point to your own S3 bucket so you can actually load the .flow file into Wrangler or access the data.\n",
    "\n",
    "After running the cell below you can open the `claims.flow` and `customers.flow` files and export the data to S3 or you can continue the guide using the provided `data/claims_preprocessed.csv` and `data/customers_preprocessed.csv` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_flow_template_file = \"flows/claims_flow_template\"\n",
    "\n",
    "with open(claims_flow_template_file, \"r\") as f:\n",
    "    variables = {\"bucket\": bucket, \"prefix\": prefix}\n",
    "    template = string.Template(f.read())\n",
    "    claims_flow = template.substitute(variables)\n",
    "    claims_flow = json.loads(claims_flow)\n",
    "\n",
    "with open(\"flows/claims.flow\", \"w\") as f:\n",
    "    json.dump(claims_flow, f)\n",
    "\n",
    "customers_flow_template_file = \"flows/customers_flow_template\"\n",
    "\n",
    "with open(customers_flow_template_file, \"r\") as f:\n",
    "    variables = {\"bucket\": bucket, \"prefix\": prefix}\n",
    "    template = string.Template(f.read())\n",
    "    customers_flow = template.substitute(variables)\n",
    "    customers_flow = json.loads(customers_flow)\n",
    "\n",
    "with open(\"flows/customers.flow\", \"w\") as f:\n",
    "    json.dump(customers_flow, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload raw data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(\n",
    "    Filename=\"flows/claims.flow\", Bucket=bucket, Key=f\"{prefix}/flow/claims.flow\"\n",
    ")\n",
    "s3_client.upload_file(\n",
    "    Filename=\"flows/customers.flow\", Bucket=bucket, Key=f\"{prefix}/flow/customers.flow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "### Next Notebook: [Train, Check Bias, Tune, Record Lineage, Register Model](./04-Lineage-Train-Assess-Bias-Tune-Registry.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
