{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangler Processing Job for Claims Dataset\n",
    "\n",
    "This notebook executes your Data Wrangler Flow `claims.flow` on the entire dataset using a SageMaker \n",
    "Processing Job and will save the processed data to S3.\n",
    "\n",
    "This notebook saves data from the step `Cast Single Data Type` from `Source: Claims.Csv`. To save from a different step, go to Data Wrangler \n",
    "to select a new step to export. \n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Inputs and Outputs](#Inputs-and-Outputs)\n",
    "1. [Run Processing Job](#Run-Processing-Job)\n",
    "   1. [Job Configurations](#Job-Configurations)\n",
    "   1. [Create Processing Job](#Create-Processing-Job)\n",
    "   1. [Job Status & S3 Output Location](#Job-Status-&-S3-Output-Location)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![train-assess-tune-register](./images/claimsprocessing.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading stored variables\n",
    "If you ran this notebook before, you may want to re-use the resources you aready created with AWS. Run the cell below to load any prevously created variables. You should see a print-out of the existing variables. If you don't see anything printed then it's probably the first time you are running the notebook! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/secretstorage/dhcrypto.py:16: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "/opt/conda/lib/python3.7/site-packages/secretstorage/util.py:25: CryptographyDeprecationWarning: int_from_bytes is deprecated, use int.from_bytes instead\n",
      "  from cryptography.utils import int_from_bytes\n",
      "Collecting awswrangler\n",
      "  Using cached awswrangler-2.12.1-py3-none-any.whl (211 kB)\n",
      "Requirement already satisfied: openpyxl<3.1.0,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from awswrangler) (3.0.3)\n",
      "Requirement already satisfied: botocore<2.0.0,>=1.19.8 in /opt/conda/lib/python3.7/site-packages (from awswrangler) (1.23.2)\n",
      "Collecting pyarrow<5.1.0,>=2.0.0\n",
      "  Using cached pyarrow-5.0.0-cp37-cp37m-manylinux2014_x86_64.whl (23.6 MB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from awswrangler) (1.20.3)\n",
      "Collecting progressbar2<4.0.0,>=3.53.3\n",
      "  Using cached progressbar2-3.55.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting pymysql<1.1.0,>=0.9.0\n",
      "  Using cached PyMySQL-1.0.2-py3-none-any.whl (43 kB)\n",
      "Collecting opensearch-py<2.0.0,>=1.0.0\n",
      "  Using cached opensearch_py-1.0.0-py2.py3-none-any.whl (207 kB)\n",
      "Collecting redshift-connector<2.1.0,>=2.0.887\n",
      "  Using cached redshift_connector-2.0.889-py3-none-any.whl (94 kB)\n",
      "Requirement already satisfied: pandas<2.0.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from awswrangler) (1.3.4)\n",
      "Collecting jsonpath-ng<2.0.0,>=1.5.3\n",
      "  Using cached jsonpath_ng-1.5.3-py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: boto3<2.0.0,>=1.16.8 in /opt/conda/lib/python3.7/site-packages (from awswrangler) (1.20.2)\n",
      "Collecting requests-aws4auth<2.0.0,>=1.1.1\n",
      "  Using cached requests_aws4auth-1.1.1-py2.py3-none-any.whl (31 kB)\n",
      "Collecting pg8000<1.22.0,>=1.16.0\n",
      "  Using cached pg8000-1.21.3-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0.0,>=1.16.8->awswrangler) (0.10.0)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from boto3<2.0.0,>=1.16.8->awswrangler) (0.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /opt/conda/lib/python3.7/site-packages (from botocore<2.0.0,>=1.19.8->awswrangler) (1.26.7)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.7/site-packages (from botocore<2.0.0,>=1.19.8->awswrangler) (2.8.1)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (4.4.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (1.14.0)\n",
      "Requirement already satisfied: ply in /opt/conda/lib/python3.7/site-packages (from jsonpath-ng<2.0.0,>=1.5.3->awswrangler) (3.11)\n",
      "Requirement already satisfied: jdcal in /opt/conda/lib/python3.7/site-packages (from openpyxl<3.1.0,>=3.0.0->awswrangler) (1.4.1)\n",
      "Requirement already satisfied: et-xmlfile in /opt/conda/lib/python3.7/site-packages (from openpyxl<3.1.0,>=3.0.0->awswrangler) (1.0.1)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from opensearch-py<2.0.0,>=1.0.0->awswrangler) (2021.10.8)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas<2.0.0,>=1.2.0->awswrangler) (2019.3)\n",
      "Collecting scramp>=1.4.1\n",
      "  Using cached scramp-1.4.1-py3-none-any.whl (8.5 kB)\n",
      "Collecting python-utils>=2.3.0\n",
      "  Using cached python_utils-2.5.6-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.887->awswrangler) (20.1)\n",
      "Requirement already satisfied: lxml>=4.6.2 in /opt/conda/lib/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.887->awswrangler) (4.6.4)\n",
      "Requirement already satisfied: requests<2.26.1,>=2.23.0 in /opt/conda/lib/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.887->awswrangler) (2.26.0)\n",
      "Collecting pytz>=2017.3\n",
      "  Using cached pytz-2021.3-py2.py3-none-any.whl (503 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.7.0 in /opt/conda/lib/python3.7/site-packages (from redshift-connector<2.1.0,>=2.0.887->awswrangler) (4.8.2)\n",
      "Requirement already satisfied: soupsieve>=1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4<5.0.0,>=4.7.0->redshift-connector<2.1.0,>=2.0.887->awswrangler) (1.9.5)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests<2.26.1,>=2.23.0->redshift-connector<2.1.0,>=2.0.887->awswrangler) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<2.26.1,>=2.23.0->redshift-connector<2.1.0,>=2.0.887->awswrangler) (2.8)\n",
      "Collecting asn1crypto>=1.4.0\n",
      "  Using cached asn1crypto-1.4.0-py2.py3-none-any.whl (104 kB)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->redshift-connector<2.1.0,>=2.0.887->awswrangler) (2.4.6)\n",
      "Installing collected packages: asn1crypto, scramp, pytz, python-utils, requests-aws4auth, redshift-connector, pymysql, pyarrow, progressbar2, pg8000, opensearch-py, jsonpath-ng, awswrangler\n",
      "  Attempting uninstall: asn1crypto\n",
      "    Found existing installation: asn1crypto 1.3.0\n",
      "    Uninstalling asn1crypto-1.3.0:\n",
      "      Successfully uninstalled asn1crypto-1.3.0\n",
      "  Attempting uninstall: pytz\n",
      "    Found existing installation: pytz 2019.3\n",
      "    Uninstalling pytz-2019.3:\n",
      "      Successfully uninstalled pytz-2019.3\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 6.0.0\n",
      "    Uninstalling pyarrow-6.0.0:\n",
      "      Successfully uninstalled pyarrow-6.0.0\n",
      "Successfully installed asn1crypto-1.4.0 awswrangler-2.12.1 jsonpath-ng-1.5.3 opensearch-py-1.0.0 pg8000-1.21.3 progressbar2-3.55.0 pyarrow-5.0.0 pymysql-1.0.2 python-utils-2.5.6 pytz-2021.3 redshift-connector-2.0.889 requests-aws4auth-1.1.1 scramp-1.4.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "!pip install awswrangler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "sagemaker_client = boto_session.client(\"sagemaker\")\n",
    "sess = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_client\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n",
      "bucket             -> 'sagemaker-us-east-1-875692608981'\n",
      "prefix             -> 'fraud-detect-demo'\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======> output_paths\n",
    "processing_dir = \"/opt/ml/processing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Outputs\n",
    "\n",
    "The below settings configure the inputs and outputs for the flow export.\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Configurable Settings </strong>\n",
    "\n",
    "In <b>Input - Source</b> you can configure the data sources that will be used as input by Data Wrangler\n",
    "\n",
    "1. For S3 sources, configure the source attribute that points to the input S3 prefixes\n",
    "2. For all other sources, configure attributes like query_string, database in the source's \n",
    "<b>DatasetDefinition</b> object.\n",
    "\n",
    "If you modify the inputs the provided data must have the same schema and format as the data used in the Flow. \n",
    "You should also re-execute the cells in this section if you have modified the settings in any data sources.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.dataset_definition.inputs import AthenaDatasetDefinition, DatasetDefinition, RedshiftDatasetDefinition\n",
    "\n",
    "data_sources = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input - S3 Source: claims.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claims s3 path: s3://sagemaker-us-east-1-875692608981/fraud-detect-demo/data/raw/claims.csv\n"
     ]
    }
   ],
   "source": [
    "data_sources.append(ProcessingInput(\n",
    "    source=f\"s3://{bucket}/{prefix}/data/raw/claims.csv\", # You can override this to point to other dataset on S3\n",
    "    destination=f\"{processing_dir}/claims\",\n",
    "    input_name=\"claims\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    "))\n",
    "print(f\"Claims s3 path: s3://{bucket}/{prefix}/data/raw/claims.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output: S3 settings\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Configurable Settings </strong>\n",
    "\n",
    "1. <b>bucket</b>: you can configure the S3 bucket where Data Wrangler will save the output. The default bucket from \n",
    "the SageMaker notebook session is used. \n",
    "2. <b>flow_export_id</b>: A randomly generated export id. The export id must be unique to ensure the results do not \n",
    "conflict with other flow exports \n",
    "3. <b>s3_ouput_prefix</b>:  you can configure the directory name in your bucket where your data will be saved.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow export name: flow-18-01-42-45-3bfd28e3\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import uuid\n",
    "\n",
    "flow_export_id = f\"{time.strftime('%d-%H-%M-%S', time.gmtime())}-{str(uuid.uuid4())[:8]}\"\n",
    "flow_export_name = f\"flow-{flow_export_id}\"\n",
    "print(f\"Flow export name: {flow_export_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the inputs required by the SageMaker Python SDK to launch a processing job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading flow file from current notebook working directory: /root/mlt\n",
      "Output name: 62d710d9-a288-4004-b960-6cf452c0380c.default\n",
      "Flow S3 export result path: s3://sagemaker-us-east-1-875692608981/fraud-detect-demo/flow/output/export-flow-18-01-42-45-3bfd28e3/output\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "# name of the flow file which should exist in the current notebook working directory\n",
    "flow_file_name = \"flows/claims.flow\"\n",
    "\n",
    "# Load .flow file from current notebook working directory \n",
    "!echo \"Loading flow file from current notebook working directory: $PWD\"\n",
    "\n",
    "with open(flow_file_name) as f:\n",
    "    flow = json.load(f)\n",
    "\n",
    "# Output name is auto-generated from the select node's ID + output name from the flow file.\n",
    "output_name = (f\"{flow['nodes'][-1]['node_id']}.{flow['nodes'][-1]['outputs'][0]['name']}\")\n",
    "print(f\"Output name: {output_name}\")\n",
    "\n",
    "s3_output_prefix = f\"export-{flow_export_name}/output\"\n",
    "s3_output_path = f\"s3://{bucket}/{prefix}/flow/output/{s3_output_prefix}\"\n",
    "print(f\"Flow S3 export result path: {s3_output_path}\")\n",
    "\n",
    "processing_job_output = ProcessingOutput(\n",
    "    output_name=output_name,\n",
    "    source=f\"{processing_dir}/output\",\n",
    "    destination=s3_output_path,\n",
    "    s3_upload_mode=\"EndOfJob\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Flow to S3\n",
    "\n",
    "To use the Data Wrangler as an input to the processing job,  first upload your flow file to Amazon S3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![train-assess-tune-register](./images/uploadclaimsflow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Wrangler flow flows/claims.flow uploaded to s3://sagemaker-us-east-1-875692608981/fraud-detect-demo/data_wrangler_flows/flow-18-01-42-45-3bfd28e3.flow\n"
     ]
    }
   ],
   "source": [
    "# Upload flow to S3\n",
    "s3_client = boto3.client(\"s3\")\n",
    "s3_client.upload_file(flow_file_name, bucket, f\"{prefix}/data_wrangler_flows/{flow_export_name}.flow\", ExtraArgs={\"ServerSideEncryption\": \"aws:kms\"})\n",
    "\n",
    "flow_s3_uri = f\"s3://{bucket}/{prefix}/data_wrangler_flows/{flow_export_name}.flow\"\n",
    "\n",
    "print(f\"Data Wrangler flow {flow_file_name} uploaded to {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data Wrangler Flow is also provided to the Processing Job as an input source which we configure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow s3 path: s3://sagemaker-us-east-1-875692608981/fraud-detect-demo/data_wrangler_flows/flow-18-01-42-45-3bfd28e3.flow\n"
     ]
    }
   ],
   "source": [
    "## Input - Flow: claims.flow\n",
    "flow_input = ProcessingInput(\n",
    "    source=flow_s3_uri,\n",
    "    destination=f\"{processing_dir}/flow\",\n",
    "    input_name=\"flow\",\n",
    "    s3_data_type=\"S3Prefix\",\n",
    "    s3_input_mode=\"File\",\n",
    "    s3_data_distribution_type=\"FullyReplicated\"\n",
    ")\n",
    "print(f\"Flow s3 path: {flow_s3_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Processing Job \n",
    "## Job Configurations\n",
    "\n",
    "<div class=\"alert alert-info\"> 💡 <strong> Configurable Settings </strong>\n",
    "\n",
    "You can configure the following settings for Processing Jobs. If you change any configurations you will \n",
    "need to re-execute this and all cells below it by selecting the Run menu above and click \n",
    "<b>Run Selected Cells and All Below</b>\n",
    "\n",
    "1. IAM role for executing the processing job. \n",
    "2. A unique name of the processing job. Give a unique name every time you re-execute processing jobs\n",
    "3. Data Wrangler Container URL.\n",
    "4. Instance count, instance type and storage volume size in GB.\n",
    "5. Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "6. Network Isolation settings\n",
    "7. KMS key to encrypt output data\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![train-assess-tune-register](./images/runclaimsprocessing.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Job Name: data-wrangler-flow-processing-18-01-42-45-3bfd28e3\n",
      "Container uri: 663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris\n",
    "# IAM role for executing the processing job.\n",
    "iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "# Unique processing job name. Give a unique name every time you re-execute processing jobs\n",
    "processing_job_name = f\"data-wrangler-flow-processing-{flow_export_id}\"\n",
    "print(f\"Processing Job Name: {processing_job_name}\")\n",
    "# Data Wrangler Container URL.\n",
    "container_uri = image_uris.retrieve(framework='data-wrangler',region=region)\n",
    "print(f\"Container uri: {container_uri}\")\n",
    "\n",
    "# Processing Job Instance count and instance type.\n",
    "instance_count = 2\n",
    "instance_type = \"ml.m5.4xlarge\"\n",
    "\n",
    "# Size in GB of the EBS volume to use for storing data during processing\n",
    "volume_size_in_gb = 30\n",
    "\n",
    "# Content type for each output. Data Wrangler supports CSV as default and Parquet.\n",
    "output_content_type = \"CSV\"\n",
    "\n",
    "# Network Isolation mode; default is off\n",
    "enable_network_isolation = False\n",
    "\n",
    "# Output configuration used as processing job container arguments \n",
    "output_config = {\n",
    "    output_name: {\n",
    "        \"content_type\": output_content_type\n",
    "    }\n",
    "}\n",
    "\n",
    "# KMS key for per object encryption; default is None\n",
    "kms_key = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Processing Job\n",
    "\n",
    "To launch a Processing Job, you will use the SageMaker Python SDK to create a Processor function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  data-wrangler-flow-processing-18-01-42-45-3bfd28e3\n",
      "Inputs:  [{'InputName': 'flow', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-875692608981/fraud-detect-demo/data_wrangler_flows/flow-18-01-42-45-3bfd28e3.flow', 'LocalPath': '/opt/ml/processing/flow', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'claims', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-875692608981/fraud-detect-demo/data/raw/claims.csv', 'LocalPath': '/opt/ml/processing/claims', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': '62d710d9-a288-4004-b960-6cf452c0380c.default', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-875692608981/fraud-detect-demo/flow/output/export-flow-18-01-42-45-3bfd28e3/output', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.processing import Processor\n",
    "from sagemaker.network import NetworkConfig\n",
    "\n",
    "processor = Processor(\n",
    "    role=iam_role,\n",
    "    image_uri=container_uri,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    volume_size_in_gb=volume_size_in_gb,\n",
    "    network_config=NetworkConfig(enable_network_isolation=enable_network_isolation),\n",
    "    sagemaker_session=sess,\n",
    "    output_kms_key=kms_key\n",
    ")\n",
    "\n",
    "# Start Job\n",
    "processor.run(\n",
    "    inputs=[flow_input] + data_sources, \n",
    "    outputs=[processing_job_output],\n",
    "    arguments=[f\"--output-config '{json.dumps(output_config)}'\"],\n",
    "    wait=False,\n",
    "    logs=False,\n",
    "    job_name=processing_job_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Status & S3 Output Location\n",
    "\n",
    "Below you wait for processing job to finish. If it finishes successfully, the raw parameters used by the \n",
    "Processing Job will be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job results are saved to S3 path: s3://sagemaker-us-east-1-875692608981/fraud-detect-demo/flow/export-flow-18-01-42-45-3bfd28e3/output/data-wrangler-flow-processing-18-01-42-45-3bfd28e3\n",
      "........................................................................!"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'ProcessingInputs': [{'InputName': 'flow',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-875692608981/fraud-detect-demo/data_wrangler_flows/flow-18-01-42-45-3bfd28e3.flow',\n",
       "    'LocalPath': '/opt/ml/processing/flow',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}},\n",
       "  {'InputName': 'claims',\n",
       "   'AppManaged': False,\n",
       "   'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-875692608981/fraud-detect-demo/data/raw/claims.csv',\n",
       "    'LocalPath': '/opt/ml/processing/claims',\n",
       "    'S3DataType': 'S3Prefix',\n",
       "    'S3InputMode': 'File',\n",
       "    'S3DataDistributionType': 'FullyReplicated',\n",
       "    'S3CompressionType': 'None'}}],\n",
       " 'ProcessingOutputConfig': {'Outputs': [{'OutputName': '62d710d9-a288-4004-b960-6cf452c0380c.default',\n",
       "    'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-875692608981/fraud-detect-demo/flow/output/export-flow-18-01-42-45-3bfd28e3/output',\n",
       "     'LocalPath': '/opt/ml/processing/output',\n",
       "     'S3UploadMode': 'EndOfJob'},\n",
       "    'AppManaged': False}]},\n",
       " 'ProcessingJobName': 'data-wrangler-flow-processing-18-01-42-45-3bfd28e3',\n",
       " 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 2,\n",
       "   'InstanceType': 'ml.m5.4xlarge',\n",
       "   'VolumeSizeInGB': 30}},\n",
       " 'StoppingCondition': {'MaxRuntimeInSeconds': 86400},\n",
       " 'AppSpecification': {'ImageUri': '663277389841.dkr.ecr.us-east-1.amazonaws.com/sagemaker-data-wrangler-container:1.x',\n",
       "  'ContainerArguments': ['--output-config \\'{\"62d710d9-a288-4004-b960-6cf452c0380c.default\": {\"content_type\": \"CSV\"}}\\'']},\n",
       " 'NetworkConfig': {'EnableInterContainerTrafficEncryption': False,\n",
       "  'EnableNetworkIsolation': False},\n",
       " 'RoleArn': 'arn:aws:iam::875692608981:role/AmazonSageMaker-ExecutionRoleMLT',\n",
       " 'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:875692608981:processing-job/data-wrangler-flow-processing-18-01-42-45-3bfd28e3',\n",
       " 'ProcessingJobStatus': 'Completed',\n",
       " 'ProcessingEndTime': datetime.datetime(2021, 11, 18, 1, 49, 32, 706000, tzinfo=tzlocal()),\n",
       " 'ProcessingStartTime': datetime.datetime(2021, 11, 18, 1, 48, 0, 247000, tzinfo=tzlocal()),\n",
       " 'LastModifiedTime': datetime.datetime(2021, 11, 18, 1, 49, 33, 36000, tzinfo=tzlocal()),\n",
       " 'CreationTime': datetime.datetime(2021, 11, 18, 1, 43, 31, 814000, tzinfo=tzlocal()),\n",
       " 'ResponseMetadata': {'RequestId': '7bc21311-695e-41dd-a616-53f32ee09aa0',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '7bc21311-695e-41dd-a616-53f32ee09aa0',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '2085',\n",
       "   'date': 'Thu, 18 Nov 2021 01:49:36 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3_job_results_path = f\"s3://{bucket}/{prefix}/flow/{s3_output_prefix}/{processing_job_name}\"\n",
    "print(f\"Job results are saved to S3 path: {s3_job_results_path}\")\n",
    "\n",
    "job_result = sess.wait_for_processing_job(processing_job_name)\n",
    "job_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='aud-datasets'></a>\n",
    "\n",
    "## DataSets and Feature Types\n",
    "[overview](#all-up-overview)\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![train-assess-tune-register](./images/claimsdataframe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_dtypes = {\n",
    "    \"policy_id\": int,\n",
    "    \"incident_severity\": int,\n",
    "    \"num_vehicles_involved\": int,\n",
    "    \"num_injuries\": int,\n",
    "    \"num_witnesses\": int,\n",
    "    \"police_report_available\": int,\n",
    "    \"injury_claim\": float,\n",
    "    \"vehicle_claim\": float,\n",
    "    \"total_claim_amount\": float,\n",
    "    \"incident_month\": int,\n",
    "    \"incident_day\": int,\n",
    "    \"incident_dow\": int,\n",
    "    \"incident_hour\": int,\n",
    "    \"fraud\": int,\n",
    "    \"driver_relationship_self\": int,\n",
    "    \"driver_relationship_na\": int,\n",
    "    \"driver_relationship_spouse\": int,\n",
    "    \"driver_relationship_child\": int,\n",
    "    \"driver_relationship_other\": int,\n",
    "    \"incident_type_collision\": int,\n",
    "    \"incident_type_breakin\": int,\n",
    "    \"incident_type_theft\": int,\n",
    "    \"collision_type_front\": int,\n",
    "    \"collision_type_rear\": int,\n",
    "    \"collision_type_side\": int,\n",
    "    \"collision_type_na\": int,\n",
    "    \"authorities_contacted_police\": int,\n",
    "    \"authorities_contacted_none\": int,\n",
    "    \"authorities_contacted_fire\": int,\n",
    "    \"authorities_contacted_ambulance\": int,\n",
    "    \"event_time\": float,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Processed Data into Pandas\n",
    "\n",
    "We use the [AWS Data Wrangler library](https://github.com/awslabs/aws-data-wrangler) to load the exported \n",
    "dataset into a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import awswrangler as wr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'claims_preprocessed' (DataFrame)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>policy_id</th>\n",
       "      <th>incident_severity</th>\n",
       "      <th>num_vehicles_involved</th>\n",
       "      <th>num_injuries</th>\n",
       "      <th>num_witnesses</th>\n",
       "      <th>police_report_available</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>incident_month</th>\n",
       "      <th>...</th>\n",
       "      <th>incident_type_theft</th>\n",
       "      <th>collision_type_front</th>\n",
       "      <th>collision_type_rear</th>\n",
       "      <th>collision_type_side</th>\n",
       "      <th>collision_type_na</th>\n",
       "      <th>authorities_contacted_police</th>\n",
       "      <th>authorities_contacted_none</th>\n",
       "      <th>authorities_contacted_ambulance</th>\n",
       "      <th>authorities_contacted_fire</th>\n",
       "      <th>event_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71600.0</td>\n",
       "      <td>8913.0</td>\n",
       "      <td>80513.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6400.0</td>\n",
       "      <td>19746.0</td>\n",
       "      <td>26146.0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10400.0</td>\n",
       "      <td>11652.0</td>\n",
       "      <td>22052.0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>104700.0</td>\n",
       "      <td>11260.0</td>\n",
       "      <td>115960.0</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3400.0</td>\n",
       "      <td>27987.0</td>\n",
       "      <td>31387.0</td>\n",
       "      <td>5</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4996</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>15700.0</td>\n",
       "      <td>1494.0</td>\n",
       "      <td>17194.0</td>\n",
       "      <td>8</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4997</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>34900.0</td>\n",
       "      <td>14837.0</td>\n",
       "      <td>49737.0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>4998</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11700.0</td>\n",
       "      <td>12421.0</td>\n",
       "      <td>24121.0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>4999</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>14000.0</td>\n",
       "      <td>10991.0</td>\n",
       "      <td>24991.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637200e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>5000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3700.0</td>\n",
       "      <td>11126.0</td>\n",
       "      <td>14826.0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.637200e+09</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      policy_id  incident_severity  num_vehicles_involved  num_injuries  \\\n",
       "0             1                  0                      2             0   \n",
       "1             2                  2                      3             4   \n",
       "2             3                  0                      2             0   \n",
       "3             4                  0                      2             0   \n",
       "4             5                  1                      2             1   \n",
       "...         ...                ...                    ...           ...   \n",
       "4995       4996                  0                      2             0   \n",
       "4996       4997                  0                      2             0   \n",
       "4997       4998                  0                      1             0   \n",
       "4998       4999                  0                      3             0   \n",
       "4999       5000                  0                      1             0   \n",
       "\n",
       "      num_witnesses  police_report_available  injury_claim  vehicle_claim  \\\n",
       "0                 0                        0       71600.0         8913.0   \n",
       "1                 0                        1        6400.0        19746.0   \n",
       "2                 1                        1       10400.0        11652.0   \n",
       "3                 0                        0      104700.0        11260.0   \n",
       "4                 0                        0        3400.0        27987.0   \n",
       "...             ...                      ...           ...            ...   \n",
       "4995              3                        0       15700.0         1494.0   \n",
       "4996              1                        0       34900.0        14837.0   \n",
       "4997              0                        0       11700.0        12421.0   \n",
       "4998              2                        1       14000.0        10991.0   \n",
       "4999              0                        0        3700.0        11126.0   \n",
       "\n",
       "      total_claim_amount  incident_month  ...  incident_type_theft  \\\n",
       "0                80513.0               3  ...                    0   \n",
       "1                26146.0              12  ...                    0   \n",
       "2                22052.0              12  ...                    0   \n",
       "3               115960.0              12  ...                    0   \n",
       "4                31387.0               5  ...                    0   \n",
       "...                  ...             ...  ...                  ...   \n",
       "4995             17194.0               8  ...                    0   \n",
       "4996             49737.0               1  ...                    0   \n",
       "4997             24121.0               6  ...                    0   \n",
       "4998             24991.0               3  ...                    0   \n",
       "4999             14826.0               3  ...                    0   \n",
       "\n",
       "      collision_type_front  collision_type_rear  collision_type_side  \\\n",
       "0                        1                    0                    0   \n",
       "1                        0                    1                    0   \n",
       "2                        1                    0                    0   \n",
       "3                        0                    0                    1   \n",
       "4                        0                    0                    1   \n",
       "...                    ...                  ...                  ...   \n",
       "4995                     1                    0                    0   \n",
       "4996                     0                    0                    1   \n",
       "4997                     1                    0                    0   \n",
       "4998                     0                    1                    0   \n",
       "4999                     0                    0                    1   \n",
       "\n",
       "      collision_type_na  authorities_contacted_police  \\\n",
       "0                     0                             0   \n",
       "1                     0                             1   \n",
       "2                     0                             1   \n",
       "3                     0                             0   \n",
       "4                     0                             1   \n",
       "...                 ...                           ...   \n",
       "4995                  0                             0   \n",
       "4996                  0                             0   \n",
       "4997                  0                             0   \n",
       "4998                  0                             1   \n",
       "4999                  0                             0   \n",
       "\n",
       "      authorities_contacted_none  authorities_contacted_ambulance  \\\n",
       "0                              1                                0   \n",
       "1                              0                                0   \n",
       "2                              0                                0   \n",
       "3                              1                                0   \n",
       "4                              0                                0   \n",
       "...                          ...                              ...   \n",
       "4995                           1                                0   \n",
       "4996                           1                                0   \n",
       "4997                           1                                0   \n",
       "4998                           0                                0   \n",
       "4999                           1                                0   \n",
       "\n",
       "      authorities_contacted_fire    event_time  \n",
       "0                              0  1.637200e+09  \n",
       "1                              0  1.637200e+09  \n",
       "2                              0  1.637200e+09  \n",
       "3                              0  1.637200e+09  \n",
       "4                              0  1.637200e+09  \n",
       "...                          ...           ...  \n",
       "4995                           0  1.637200e+09  \n",
       "4996                           0  1.637200e+09  \n",
       "4997                           0  1.637200e+09  \n",
       "4998                           0  1.637200e+09  \n",
       "4999                           0  1.637200e+09  \n",
       "\n",
       "[5000 rows x 31 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ======> This is your DataFlow output path if you decide to redo the work in DataFlow on your own\n",
    "if output_content_type.upper() == \"CSV\":\n",
    "    claims_preprocessed = wr.s3.read_csv(\n",
    "        path=s3_output_path, dataset=True, dtype=claims_dtypes\n",
    "    )\n",
    "else:\n",
    "    print(f\"Unexpected output content type {output_content_type}\")\n",
    "\n",
    "%store claims_preprocessed\n",
    "claims_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 31 columns):\n",
      " #   Column                           Non-Null Count  Dtype  \n",
      "---  ------                           --------------  -----  \n",
      " 0   policy_id                        5000 non-null   int64  \n",
      " 1   incident_severity                5000 non-null   int64  \n",
      " 2   num_vehicles_involved            5000 non-null   int64  \n",
      " 3   num_injuries                     5000 non-null   int64  \n",
      " 4   num_witnesses                    5000 non-null   int64  \n",
      " 5   police_report_available          5000 non-null   int64  \n",
      " 6   injury_claim                     5000 non-null   float64\n",
      " 7   vehicle_claim                    5000 non-null   float64\n",
      " 8   total_claim_amount               5000 non-null   float64\n",
      " 9   incident_month                   5000 non-null   int64  \n",
      " 10  incident_day                     5000 non-null   int64  \n",
      " 11  incident_dow                     5000 non-null   int64  \n",
      " 12  incident_hour                    5000 non-null   int64  \n",
      " 13  fraud                            5000 non-null   int64  \n",
      " 14  driver_relationship_self         5000 non-null   int64  \n",
      " 15  driver_relationship_na           5000 non-null   int64  \n",
      " 16  driver_relationship_spouse       5000 non-null   int64  \n",
      " 17  driver_relationship_child        5000 non-null   int64  \n",
      " 18  driver_relationship_other        5000 non-null   int64  \n",
      " 19  incident_type_collision          5000 non-null   int64  \n",
      " 20  incident_type_breakin            5000 non-null   int64  \n",
      " 21  incident_type_theft              5000 non-null   int64  \n",
      " 22  collision_type_front             5000 non-null   int64  \n",
      " 23  collision_type_rear              5000 non-null   int64  \n",
      " 24  collision_type_side              5000 non-null   int64  \n",
      " 25  collision_type_na                5000 non-null   int64  \n",
      " 26  authorities_contacted_police     5000 non-null   int64  \n",
      " 27  authorities_contacted_none       5000 non-null   int64  \n",
      " 28  authorities_contacted_ambulance  5000 non-null   int64  \n",
      " 29  authorities_contacted_fire       5000 non-null   int64  \n",
      " 30  event_time                       5000 non-null   float64\n",
      "dtypes: float64(4), int64(27)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "claims_preprocessed.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a set of Pandas DataFrames that contain the customer and claim data, with the correct data types. When Dat Wrangler encodes a feature as one-hot-encoded feature, it will default to float data types for those resulting features (one feature --> many columns for the one hot encoding). \n",
    "\n",
    "<font color ='red'> Note: </font> the reason for explicitly converting the data types for categorical features generated by Data Wrangler, is to ensure they are of type integer so that Clarify will treat them as categorical variables. "
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
